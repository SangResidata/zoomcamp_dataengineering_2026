## Workflow Orchestration  

Welcome to Module 2 of the Data Engineering Zoomcamp! This week, we’ll dive into workflow orchestration using Kestra.  
Kestra is an open-source, event-driven orchestration platform that simplifies building both scheduled and event-driven workflows. By adopting Infrastructure as Code practices for data and process orchestration, Kestra enables you to build reliable workflows with just a few lines of YAML.  

## Course Structure

    2.1 - Introduction to Workflow Orchestration
    2.2 - Getting Started With Kestra
    2.3 - Hands-On Coding Project: Build ETL Data Pipelines with Kestra
    2.4 - ELT Pipelines in Kestra: Google Cloud Platform
    2.5 - Using AI for Data Engineering in Kestra
    2.6 - Bonus

## 2.1 - Introduction to Workflow Orchestration
In this section, you'll learn how to install Kestra, as well as the key concepts required to build your first workflow. Once our first workflow is built, we can extend this further by executing a Python script inside of a workflow.

### 2.1.1 What is Workflow Orchestration
Think of a music orchestra. There's a variety of different instruments. Some more than others, all with different roles when it comes to playing music. To make sure they all come together at the right time, they follow a conductor who helps the orchestra to play together.

Now replace the instruments with tools and the conductor with an orchestrator. We often have multiple tools and platforms that we need to work together. Sometimes on a routine schedule, other times based on events that happen. That's where the orchestrator comes in to help all of these tools work together.

A workflow orchestrator might do the following tasks:

    - Run workflows which contain a number of predefined steps
    - Monitor and log errors, as well as taking a number of extra steps when they occur
    - Automatically run workflows based on schedules and events

In data engineering, you often need to move data from one place, to another, sometimes with some modifications made to the data in the middle. This is where a workflow orchestrator can help out by managing these steps, while giving us visibility into it at the same time.

In this module, we're going to build our own data pipeline using ETL (Extract, Transform Load) with Kestra at the core of the operation, but first we need to understand a bit more about how Kestra works before we can get building!
You will:  

    1. Install Kestra using Docker Compose
    2. Learn the concepts of Kestra to build your first workflow
    3. Execute a Python script inside of a Kestra Flow  


### 2.1.2 What is Kestra?
Kestra is an open-source, infinitely-scalable orchestration platform that enables all engineers to manage business-critical workflows.

Kestra is a great choice for workflow orchestration:

    Build with Flow code (YAML), No-code or with the AI Copilot - flexibility in how you build your workflows
    1000+ Plugins - integrate with all the tools you use
    Support for any programming language - pick the right tool for the job
    Schedule or Event Based Triggers - have your workflows respond to data

## 2.2 Getting Started with Kestra  
In this section, you'll learn how to install Kestra, as well as the key concepts required to build your first workflow. Once our first workflow is built, we can extend this further by executing a Python script inside of a workflow.

You will:

    - Install Kestra using Docker Compose
    - Learn the concepts of Kestra to build your first workflow
    - Execute a Python script inside of a Kestra Flow  

### 2.2.1 Installing Kestra  
o install Kestra, we are going to use Docker Compose. We already have a Postgres database set up, along with pgAdmin from Module 1. We can continue to use these with Kestra but we'll need to make a few modifications to our Docker Compose file.

Use this example Docker Compose file to correctly add the 2 new services and set up the volumes correctly.

Add information about setting a username and password.

We'll set up Kestra using Docker Compose containing one container for the Kestra server and another for the Postgres database:  

cd 02-workflow-orchestration
docker compose up -d  

Note: Check that pgAdmin isn't running on the same ports as Kestra. If so, check out the FAQ at the bottom of the README.

Once the container starts, you can access the Kestra UI at http://localhost:8080.

To shut down Kestra, go to the same directory and run the following command:  

docker compose down  

### Add Flows to Kestra  
Flows can be added to Kestra by copying and pasting the YAML directly into the editor, or by adding via Kestra's API. See below for adding programmatically.
Add Flows to Kestra programmatically

https://youtu.be/wgPxC4UjoLM  

### 2.2.2 - Kestra Concepts  
To start building workflows in Kestra, we need to understand a number of concepts.

    Flow - a container for tasks and their orchestration logic.
    Tasks - the steps within a flow.
    Inputs - dynamic values passed to the flow at runtime.
    Outputs - pass data between tasks and flows.
    Triggers - mechanism that automatically starts the execution of a flow.
    Execution - a single run of a flow with a specific state.
    Variables - key–value pairs that let you reuse values across tasks.
    Plugin Defaults - default values applied to every task of a given type within one or more flows.
    Concurrency - control how many executions of a flow can run at the same time.

While there are more concepts used for building powerful workflows, these are the ones we're going to use to build our data pipelines.

The flow 01_hello_world.yaml showcases all of these concepts inside of one workflow:

    The flow has 5 tasks: 3 log tasks and a sleep task
    The flow takes an input called name.
    There is a variable that takes the name input to generate a full welcome message.
    An output is generated from the return task and is logged in a later log task.
    There is a trigger to execute this flow every day at 10am.
    Plugin Defaults are used to make both log tasks send their messages as ERROR level.
    We have a concurrency limit of 2 executions. Any further ones made while 2 are running will fail.




